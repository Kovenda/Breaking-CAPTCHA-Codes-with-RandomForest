---
jupyter:
  colab:
    name: Project 2_Dec6RF_Final.ipynb
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.8.5
  nbformat: 4
  nbformat_minor: 1
---

::: {.cell .markdown id="T2jp9nmobwGj"}
!pip install imutils
:::

::: {.cell .markdown id="CQ1BBGcu1tf8"}
The Captcha image consists of four characters that are either a string
of letters or a combination of letters with numbers. In some of the
combinations, the captcha characters contain all letters but they are
not aligned cleanly. Some of the letters are joined together or
diagonally aligned, therefore correctly separating them in order to make
them easily recognised as letters is one of the tasks at hand. In the
instances of Captcha characters that contain both numbers and letters,
the position of the number is not guaranteed and the letters are
slanted.

The expected outcome is to correctly identify each of the Captcha
characters from the image. We are doing this inorder to measure the
efficiency of the neural networks to correctly identify the Captcha
characters. The label variable is the letter or number from the image of
the separated Captcha characters.
:::

::: {.cell .code id="SLZy52CMbwGo"}
``` {.python}
# Common imports
import numpy as np
import os


# to make this notebook's output stable across runs
np.random.seed(42)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
```
:::

::: {.cell .markdown id="9a0LhN00bwGr"}
!pip install opencv
:::

::: {.cell .markdown id="3de37QhCbwGr"}
conda install -c conda-forge/label/gcc7 opencv
:::

::: {.cell .markdown id="JFYAeXLubwGs"}
conda install -c menpo opencv
:::

::: {.cell .markdown id="G8chS8aXbwGs"}
import zipfile with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:
zip_ref.extractall(\"targetdir\")
:::

::: {.cell .code id="IFd-2NPUbwGt"}
``` {.python}
 import os
```
:::

::: {.cell .code id="mxeVAgY1bwGu" outputId="ebace716-a25f-4139-9aa4-3a308c089626"}
``` {.python}
os.getcwd()
```

::: {.output .execute_result execution_count="3"}
    '/home/LC/mbuako01/DS 420 Project 2'
:::
:::

::: {.cell .markdown id="aFmmhCAAbwGw"}
from PIL import Image import os, os.path

imgs = \[\] path = \"/home/LC/chakth01/Neural networks\" valid_images =
\[\".jpg\",\".gif\",\".png\",\".tga\"\] for f in os.listdir(path): ext =
os.path.splitext(f)\[1\] if ext.lower() not in valid_images: continue
imgs.append(Image.open(os.path.join(path,f)))
:::

::: {.cell .code id="71AC9h0gbwGy"}
``` {.python}
#image_file_Path ="/home/LC/chakth01/Neural networks/targetdir/captcha_images/"
image_file_Path ="/home/LC/mbuako01/DS 420 Project 2/imageFolder/captcha_images/"
```
:::

::: {.cell .code id="dK1TlKYZbwGz" outputId="2a4697a2-cd73-4bff-cd98-126d1e9bb4e8"}
``` {.python}
image_file_Path
```

::: {.output .execute_result execution_count="5"}
    '/home/LC/mbuako01/DS 420 Project 2/imageFolder/captcha_images/'
:::
:::

::: {.cell .code id="-UtxDAuPbwG0"}
``` {.python}
def read_image(image_file_path):
    """Read in an image file."""
    bgr_img = cv2.imread(image_file_path)
    b,g,r = cv2.split(bgr_img)       # get b,g,r
    rgb_img = cv2.merge([r,g,b])     # switch it to rgb
    
    return rgb_img 
```
:::

::: {.cell .code id="9iG-P0ibbwG1"}
``` {.python}
import cv2
import imutils
import numpy as np
import os
from imutils import paths
import pandas as pd
```
:::

::: {.cell .code id="BRtzwowqbwG1"}
``` {.python}
images = []
labels = []
```
:::

::: {.cell .code id="yTZzemJ6bwG2"}
``` {.python}
for image_file_path in imutils.paths.list_images(image_file_Path):
    image_file = read_image(image_file_path)
    label = image_file_path.split('/')[7]
    images.append(image_file)
    labels.append(label)
    
```
:::

::: {.cell .code id="bNfwNLu_bwG2"}
``` {.python}
#images
```
:::

::: {.cell .code id="G3ZO9qiUbwG2"}
``` {.python}
#labels
```
:::

::: {.cell .code id="JnO5bF_bbwG3"}
``` {.python}
newLabels = []

for label in labels:
    labelHere = label.split('.')[0]
    newLabels.append(labelHere)
```
:::

::: {.cell .code id="bNy5T4xNbwG3"}
``` {.python}
#newLabels
```
:::

::: {.cell .code id="VOhkO7FNbwG3"}
``` {.python}
images = np.array(images)
#images4Plot = np.array(images, dtype="float") / 255.0
labels = np.array(newLabels)
```
:::

::: {.cell .code id="3ZJnRLnBbwG4" outputId="395741de-3175-41e4-c151-63372d578e7f"}
``` {.python}
print(labels)
```

::: {.output .stream .stdout}
    ['GU3J' 'EFTZ' 'BVBR' ... 'QZW8' 'WXNY' 'R55D']
:::
:::

::: {.cell .code id="tShiWXxObwG4"}
``` {.python}
#print(images)
```
:::

::: {.cell .markdown}
A dataset of 9,955 of unique CAPTCHA images each with its label as the
filename was used for this research. However, machine learning
classification requires a one-to-many relationship between a label and
in this context the CAPTCHA images. Therefore, uniqueness of the CAPTCHA
images is problematic for a machine learning process.
:::

::: {.cell .code id="9jUbdAcUbwG4" outputId="4ec88a9c-e274-4242-ee12-5f8bd225c592"}
``` {.python}
images.shape
```

::: {.output .execute_result execution_count="17"}
    (9955, 24, 72, 3)
:::
:::

::: {.cell .code id="GZ22L_yObwG5" outputId="b12af1d0-fd1b-4329-d237-504505565885"}
``` {.python}
some_digit = images[300]
#Some_digit_image = some_digit.reshape(24, 72, 3)
plt.imshow(some_digit, cmap = mpl.cm.binary,
           interpolation="nearest")
plt.axis("off")


plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/d0de6d4167aae20e07f8c9576a487b463410295a.png)
:::
:::

::: {.cell .code id="Vbye-kkvbwG5" outputId="a0850388-aa94-473d-9721-30acade0f876"}
``` {.python}
labels[300]
```

::: {.output .execute_result execution_count="19"}
    'DB3W'
:::
:::

::: {.cell .code id="gpZn5zddbwG6"}
``` {.python}
import os
import os.path
import cv2
import glob
import imutils
```
:::

::: {.cell .code id="hRmPmAfVbwG6"}
``` {.python}
def pureBlackWhiteConversionThreshold(image):
    # Add some extra padding around the image
    imagePadded = cv2.copyMakeBorder(image, 8, 8, 8, 8, cv2.BORDER_REPLICATE)
    gray = cv2.cvtColor(imagePadded, cv2.COLOR_RGB2GRAY)
    # threshold the image (convert it to pure black and white)
    imagethresholded = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]    
    return imagethresholded 
```
:::

::: {.cell .code id="YDu5ghf-bwG6"}
``` {.python}
def pureBlackWhiteConversionOGImage(image):
    # Add some extra padding around the image
    imagePadded = cv2.copyMakeBorder(image, 8, 8, 8, 8, cv2.BORDER_REPLICATE)
    gray = cv2.cvtColor(imagePadded, cv2.COLOR_RGB2GRAY)
       
    return gray 
```
:::

::: {.cell .code id="c2aDj9nUbwG6"}
``` {.python}
padded_ThreshImage300 = pureBlackWhiteConversionThreshold(images[300])
```
:::

::: {.cell .code id="n5agxokjbwG7" outputId="3a631d13-8d9d-48d5-dd52-a0fe2ea3be61"}
``` {.python}
some_digit = padded_ThreshImage300

plt.imshow(some_digit, cmap = mpl.cm.binary,
           interpolation="nearest")
plt.axis("off")


plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/8995982a1771d8fa3139a964a1f0de4b7efcd475.png)
:::
:::

::: {.cell .code id="PUMzfShXbwG7"}
``` {.python}
def regionsOfLetters(image):
    
     # find the contours (continuous blobs of pixels) the image
    contours = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Hack for compatibility with different OpenCV versions
    contours = contours[0] if imutils.is_cv2() else contours[1]

    letter_image_regions = []
    
    # Now we can loop through each of the four contours and extract the letter
    # inside of each one
    for contour in contours:
        # Get the rectangle that contains the contour
        (x, y, w, h) = cv2.boundingRect(contour)

        # Compare the width and height of the contour to detect letters that
        # are conjoined into one chunk
        if w / h > 1.25:
            # This contour is too wide to be a single letter!
            # Split it in half into two letter regions!
            half_width = int(w / 2)
            letter_image_regions.append((x, y, half_width, h))
            letter_image_regions.append((x + half_width, y, half_width, h))
        else:
            # This is a normal letter by itself
            letter_image_regions.append((x, y, w, h))
    # If we found more or less than 4 letters in the captcha, our letter extraction
    # didn't work correcly. Skip the image instead of saving bad training data!
    #if len(letter_image_regions) != 4:
       # continue

    # Sort the detected letter images based on the x coordinate to make sure
    # we are processing them from left-to-right so we match the right image
    # with the right letter
    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])
    
    return letter_image_regions 
```
:::

::: {.cell .code id="MNrpgfwtbwG8" outputId="13e1e441-cbd0-451a-dd0f-2d9a9f172dfd"}
``` {.python}
letter_image_regions = regionsOfLetters(padded_ThreshImage300)
letter_image_regions
```

::: {.output .execute_result execution_count="26"}
    [(16, 12, 10, 13), (31, 12, 9, 13), (46, 12, 7, 12), (64, 12, 14, 13)]
:::
:::

::: {.cell .markdown}
To deal with the uniqueness problem of the dataset, the solution was to
separate the CAPTCHA images into the individual 4 characters that make
up the CHAPTCHA image. This was to make each character into its own
image. The resulting dataset has 39,754 images with one character per
image. The new dataset satisfies the one-to-many relationship between
the images and the following 32 characters labels {\'2\', \'3\', \'4\',
\'5\', \'6\', \'7\', \'8\', \'9\', \'A\', \'B\', \'C\', \'D\', \'E\',
\'F\', \'G\', \'H\', \'J\', \'K\', \'L\', \'M\', \'N\', \'P\', \'Q\',
\'R\', \'S\', \'T\', \'U\', \'V\', \'W\', \'X\', \'Y\', \'Z\'}
:::

::: {.cell .code id="MSq8_MbEbwG8"}
``` {.python}
def extractLetters(letter_image_regions, image):
    # Save out each letter as a single image
    letter_images =[]
    for letter_bounding_box in letter_image_regions:
        # Grab the coordinates of the letter in the image
        x, y, w, h = letter_bounding_box

        # Extract the letter from the original image with a 2-pixel margin around the edge
        letter_image = image[y - 2:y + h + 2, x - 2:x + w + 2]
        #image_file1 = read_image(letter_image)
        letter_images.append(letter_image)
    return letter_images 
    
```
:::

::: {.cell .code id="9F6-bJaNbwG8"}
``` {.python}
grayScaleImage = pureBlackWhiteConversionOGImage(images[300])
letter_image_List = extractLetters(letter_image_regions,grayScaleImage)
```
:::

::: {.cell .code id="sHNKrFB3bwG8"}
``` {.python}
checkImage = letter_image_List[3]
```
:::

::: {.cell .code id="QT2qLw9rbwG9" outputId="3adaaf7a-3f57-4f45-cf28-a360b5bbd7bb"}
``` {.python}
some_digit = checkImage
#Some_digit_image = some_digit.reshape(24, 72, 3)
plt.imshow(some_digit, cmap = mpl.cm.binary,
           interpolation="nearest")
plt.axis("off")


plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/2492e78e7724e2c4241d4148b15603de796bd78a.png)
:::
:::

::: {.cell .code id="6-YO9ddmbwG9" outputId="04c3f444-4413-47a4-b505-5497fead2030"}
``` {.python}
len(labels)
```

::: {.output .execute_result execution_count="31"}
    9955
:::
:::

::: {.cell .code id="CoBGgQRGbwG9" outputId="112e3d97-be66-4e28-d799-2334ae01664d"}
``` {.python}
len(images)
```

::: {.output .execute_result execution_count="32"}
    9955
:::
:::

::: {.cell .code id="P7ndUHvEbwG-" outputId="a972a3c7-4c02-4b86-cc85-889e640913ff"}
``` {.python}
images.shape
```

::: {.output .execute_result execution_count="33"}
    (9955, 24, 72, 3)
:::
:::

::: {.cell .code id="ao7hhpoNbwG-"}
``` {.python}
def expand2square(image):
    desired_size = 28
    im = image
    old_size = im.shape[:2] # old_size is in (height, width) format

    ratio = float(desired_size)/max(old_size)
    new_size = tuple([int(x*ratio) for x in old_size])

    # new_size should be in (width, height) format

    im = cv2.resize(im, (new_size[1], new_size[0]))

    delta_w = desired_size - new_size[1]
    delta_h = desired_size - new_size[0]
    top, bottom = delta_h//2, delta_h-(delta_h//2)
    left, right = delta_w//2, delta_w-(delta_w//2)

    color = [255, 255, 255]
    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,
        value=color)
    return new_im
```
:::

::: {.cell .code id="XK3fsO1ZbwG-"}
``` {.python}
letterImages = []
letterImageLabels = []


# loop over the unseparated image list
for label, image in zip(labels, images):
    
    padded_ThreshImage300 = pureBlackWhiteConversionThreshold(image)
    letter_image_regions = regionsOfLetters(padded_ThreshImage300)
    grayScaleImage = pureBlackWhiteConversionOGImage(image)
    letter_image_List = extractLetters(letter_image_regions,grayScaleImage)
     #image_reshape = letter_bounding_box.reshape(L0, L1)
    for letter_bounding_box, letter_text in zip(letter_image_List, label): 
        L0=letter_bounding_box.shape[0]
        L1=letter_bounding_box.shape[1]
       
        imageResize = expand2square(letter_bounding_box)
        letterImages.append(imageResize)
        letterImageLabels.append(letter_text)
    
        
        
```
:::

::: {.cell .code id="szT8uHmEbwG-" outputId="6dfbbc04-a683-4df2-e996-967496159d55"}
``` {.python}
len(letterImages)
```

::: {.output .execute_result execution_count="36"}
    39754
:::
:::

::: {.cell .code id="p61FxcovbwG_" outputId="b5f92c33-c6a8-4fa6-bab0-f12de894a81d"}
``` {.python}
len(letterImageLabels)
```

::: {.output .execute_result execution_count="37"}
    39754
:::
:::

::: {.cell .code id="N083gkWebwG_" outputId="84f37d77-be22-4b0d-d71e-afe2e501d57e"}
``` {.python}
displayDigit = letterImages[609]

plt.imshow(displayDigit, cmap = mpl.cm.binary,
           interpolation="nearest")
plt.axis("off")


plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/ee0447a3ff48b1fb5e393400494480c4aaf221bf.png)
:::
:::

::: {.cell .code id="Nvsg_tE_bwG_" outputId="b2f35a63-535d-4ea2-f85a-4c333ff0970a"}
``` {.python}
letterImages[609].shape
```

::: {.output .execute_result execution_count="39"}
    (28, 28)
:::
:::

::: {.cell .code id="WRz9__bQbwG_"}
``` {.python}
images = np.array(letterImages)
labels = np.array(letterImageLabels)
```
:::

::: {.cell .code id="W3asXyDibwHA" outputId="8d518b27-cca2-4fb4-e1ca-a0064894cc15"}
``` {.python}
 images.shape
```

::: {.output .execute_result execution_count="41"}
    (39754, 28, 28)
:::
:::

::: {.cell .code id="Q1GyJ0bvbwHA"}
``` {.python}
def plot_digit(image):
    some_digit = image
    plt.imshow(some_digit, cmap = mpl.cm.binary,
               interpolation="nearest")
    plt.axis("off")


    plt.show()
```
:::

::: {.cell .code id="LXNNhpYRbwHA"}
``` {.python}
# EXTRA
def plot_digits(instances, images_per_row=10, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size,size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap = mpl.cm.binary, **options)
    plt.axis("off")
```
:::

::: {.cell .code id="2aB_pbWKbwHA" outputId="185f22d9-0ad9-4d62-b2da-abe54a48b15e"}
``` {.python}
index, = np.where(labels == 'F')
index
```

::: {.output .execute_result execution_count="44"}
    array([    5,    32,    94, ..., 39720, 39722, 39736])
:::
:::

::: {.cell .code id="yuSuYpUxbwHB" outputId="11b33a94-eeec-41d0-a6a7-1410cb744596"}
``` {.python}
plt.figure(figsize=(15, 15))
example_images = np.r_[images[[14,39,51,39702,39752]], 
                       images[[7,56,61,39703,39714]],
                       images[[45,   198,   352,39705, 39719]], 
                       images[[2,    12,    52, 39698, 39712]], 
                       images[[3,    26,    87, 39612, 39619]]]

example_images
plot_digits(example_images, images_per_row=5)
#save_fig("more_digits_plot")
#plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/deb47268075c5456f794f88455f6f8a4307c4491.png)
:::
:::

::: {.cell .markdown id="fc-yaA6zbwHB"}
## 4. Prepare Data for RandomForest Model {#4-prepare-data-for-randomforest-model}
:::

::: {.cell .code id="sbU2bbfUbwHB" outputId="17366e1f-f8e1-4d77-aa5c-ab8cebebdb89"}
``` {.python}
images3DTo2D = images.reshape(39754, 28 * 28)
images3DTo2D.shape
```

::: {.output .execute_result execution_count="46"}
    (39754, 784)
:::
:::

::: {.cell .code id="mLoUmlmObwHB" outputId="14af4131-026c-4194-b1eb-dba328a9685a"}
``` {.python}
X, y = images3DTo2D, labels
X.shape
```

::: {.output .execute_result execution_count="47"}
    (39754, 784)
:::
:::

::: {.cell .code id="V4gC1x_bbwHC" outputId="4086c44b-a4e9-4010-8d65-c91a55b99bc2"}
``` {.python}
np.unique(y)
```

::: {.output .execute_result execution_count="48"}
    array(['2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E',
           'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T',
           'U', 'V', 'W', 'X', 'Y', 'Z'], dtype='<U1')
:::
:::

::: {.cell .markdown id="DxshLD2qbwHC"}
## 5. Train test split {#5-train-test-split}
:::

::: {.cell .markdown}
The image data was split into training and test set data, with the
training set having 80% of the data and the remaining 20% was kept for
the evaluation of model performance on the test set. The data was then
fit to the random forest model.
:::

::: {.cell .code id="_7PRN0oPbwHC"}
``` {.python}
from sklearn.model_selection import train_test_split

(X_train, X_test, y_train, y_test) = train_test_split(
    X, y, test_size=0.2, random_state=11
)
```
:::

::: {.cell .code id="Z6v4PNOWbwHC"}
``` {.python}
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)
```
:::

::: {.cell .code id="AXUfydAQbwHC" outputId="b2ae5d4d-6fc0-4836-9d31-17db32d6eb6f"}
``` {.python}
forest_clf.fit(X_train, y_train)
```

::: {.output .execute_result execution_count="51"}
    RandomForestClassifier(n_estimators=10, random_state=42)
:::
:::

::: {.cell .markdown id="08Wd56VnbwHD"}
### Random Forest training set Performance
:::

::: {.cell .code id="8tJVuJpibwHD"}
``` {.python}
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
```
:::

::: {.cell .code id="XTOzniyibwHD"}
``` {.python}
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

```
:::

::: {.cell .code id="M4bp-sghbwHD"}
``` {.python}
def plot_confusion_matrix(matrix):
    """If you prefer color and a colorbar"""
    fig = plt.figure(figsize=(5,5))
    ax = fig.add_subplot(111)
    cax = ax.matshow(matrix)
    fig.colorbar(cax)
```
:::

::: {.cell .code id="x0Xoc5FQbwHD"}
``` {.python}
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import precision_recall_curve, roc_curve
from sklearn.preprocessing import label_binarize

import matplotlib.pyplot as plt
#%matplotlib inline


# Binarize the output
y_trainBinarize = label_binarize(y_train, classes=np.unique(y))
n_classes = y_trainBinarize.shape[1]
n_classes

clf = OneVsRestClassifier(forest_clf)
clf.fit(X_train, y_trainBinarize)

y_score = clf.predict_proba(X_train)
```
:::

::: {.cell .code id="6wnm8SzzbwHE" outputId="b4f201fe-3ba2-4c16-8a17-3abd8c08ee18"}
``` {.python}
y_train_pred = cross_val_predict(clf, X_train_scaled, y_train, cv=3)
conf_mx = confusion_matrix(y_train, y_train_pred)
conf_mx
```

::: {.output .execute_result execution_count="56"}
    array([[940,   0,   1, ...,   2,   0,   0],
           [  0, 980,   0, ...,   1,   2,   1],
           [  0,   0, 922, ...,   0,   1,   2],
           ...,
           [  2,   0,   0, ..., 973,   0,   1],
           [  1,   0,   1, ...,   0, 981,   1],
           [  2,   0,   0, ...,   1,   2, 999]])
:::
:::

::: {.cell .code id="tugoKKUabwHE" outputId="3476fc9a-24de-45c5-87d4-fbf7912bc15e"}
``` {.python}
plt.matshow(conf_mx, cmap=plt.cm.gray)

plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/5365f1f1c87b6a653510fb09a5bb997364b04f31.png)
:::
:::

::: {.cell .code id="qtXlFxAjbwHE"}
``` {.python}
row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums
```
:::

::: {.cell .code id="kucWfEhabwHE" outputId="c5c16155-ca9c-4266-80f5-12db17a0d879"}
``` {.python}
np.fill_diagonal(norm_conf_mx, 0)
plt.matshow(norm_conf_mx, cmap=plt.cm.gray)

plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/778e962171f372a179601b5905b0c84a4ebe1885.png)
:::
:::

::: {.cell .code id="1o0uQINAbwHE" outputId="27f9c102-3e90-4ff8-ee9d-9ada6e303889"}
``` {.python}
accuracy_score(y_train, y_train_pred)
```

::: {.output .execute_result execution_count="60"}
    0.9795616765713926
:::
:::

::: {.cell .code id="0ktP07WbbwHF" outputId="02531a61-9240-487a-f4f2-94c101126852"}
``` {.python}
precision_score(y_train, y_train_pred, average='micro')
```

::: {.output .execute_result execution_count="61"}
    0.9795616765713926
:::
:::

::: {.cell .code id="M8mE4xZNbwHF" outputId="a257a958-db3c-4b17-c612-5d4a877fcc71"}
``` {.python}
recall_score(y_train, y_train_pred, average='micro')
```

::: {.output .execute_result execution_count="62"}
    0.9795616765713926
:::
:::

::: {.cell .code id="5giczZ8IbwHF" outputId="de325b5d-661b-4517-f9ac-40824ea19960"}
``` {.python}
f1_score(y_train, y_train_pred, average='micro')
```

::: {.output .execute_result execution_count="63"}
    0.9795616765713926
:::
:::

::: {.cell .code id="DgQQ52E9bwHF" outputId="46430aa8-50f3-4c4d-8b12-148e2ade2cd1"}
``` {.python}
# precision recall curve
precision = dict()
recall = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_trainBinarize[:, i],
                                                        y_score[:, i])
    plt.plot(recall[i], precision[i], lw=2)
    
plt.xlabel("recall")
plt.ylabel("precision")
#plt.legend(loc="upper left")
plt.title("precision vs. recall curve")
plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/6216f1e130aafc561c1cb81b4dc53af4c6351c18.png)
:::
:::

::: {.cell .markdown id="4wQcrY2YbwHF"}
### Training set ROC curves
:::

::: {.cell .code id="vqWLeajKbwHG" outputId="94912567-a1da-4382-b081-f3667d11eadc"}
``` {.python}
# roc curve
fpr = dict()
tpr = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_trainBinarize[:, i],
                                  (y_score[:, i]))
    plt.plot(fpr[i], tpr[i], lw=2)

plt.xlabel("false positive rate")
plt.ylabel("true positive rate")

plt.title("ROC curve")
plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/c4f0309f843c8a99a6d4b59884e7874273bc8c20.png)
:::
:::

::: {.cell .markdown id="PKkxXEIXbwHG"}
The random forest model did very well predicting the individual
characters from the CAPTCHA images with an accuracy score, precision
score, recall score and f1 score, all of 97.956%. The precision vs
recall and ROC curves shows that the model is predicting most characters
correctly.
:::

::: {.cell .markdown id="krr_JpTgbwHG"}
### Random Forest test set Performance
:::

::: {.cell .code id="m21iWB5PbwHG" outputId="48f6e484-8d09-407d-f20f-3bd23d557b4d"}
``` {.python}
X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))
y_test_pred = cross_val_predict(clf, X_test_scaled, y_test, cv=3)
conf_mx = confusion_matrix(y_test, y_test_pred)
conf_mx
```

::: {.output .execute_result execution_count="66"}
    array([[255,   0,   0, ...,   0,   0,   0],
           [  0, 213,   0, ...,   0,   0,   0],
           [  0,   0, 272, ...,   0,   0,   0],
           ...,
           [  1,   0,   0, ..., 235,   1,   0],
           [  0,   0,   0, ...,   1, 247,   0],
           [  2,   0,   1, ...,   0,   1, 229]])
:::
:::

::: {.cell .code id="fQMBobAfbwHG" outputId="f25be45d-5a58-40d7-a69c-d773ce45b078"}
``` {.python}
plt.matshow(conf_mx, cmap=plt.cm.gray)

plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/02ed9d25de4863f789686ec3e9ab332e769bdbc3.png)
:::
:::

::: {.cell .code id="SJ7A3ynibwHH"}
``` {.python}
row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums
```
:::

::: {.cell .code id="gnCMxKy2bwHH" outputId="a4077bfb-d745-4155-8228-a8e20739fb14"}
``` {.python}
np.fill_diagonal(norm_conf_mx, 0)
plt.matshow(norm_conf_mx, cmap=plt.cm.gray)

plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/ed5faee1a695e195ac14ddef46077aef664a31bc.png)
:::
:::

::: {.cell .code id="m8yaKlxnbwHH" outputId="d83b042e-d6be-4efa-b34a-f4e44f3bf43f"}
``` {.python}
accuracy_score(y_test, y_test_pred)
```

::: {.output .execute_result execution_count="70"}
    0.9752232423594517
:::
:::

::: {.cell .code id="kr7RPvVKbwHH" outputId="a65c5a9a-84c0-45da-f667-82f26d61e0dd"}
``` {.python}
precision_score(y_test, y_test_pred, average='micro')
```

::: {.output .execute_result execution_count="71"}
    0.9752232423594517
:::
:::

::: {.cell .code id="rdflafzEbwHH" outputId="d341b9a5-eeb0-4f77-85a8-0924f46ab897"}
``` {.python}
recall_score(y_test, y_test_pred, average='micro')
```

::: {.output .execute_result execution_count="72"}
    0.9752232423594517
:::
:::

::: {.cell .code id="6NQBTzK9bwHI" outputId="98f2a4aa-67eb-413c-a77b-98d24fec22ae"}
``` {.python}
f1_score(y_test, y_test_pred, average='micro')
```

::: {.output .execute_result execution_count="73"}
    0.9752232423594517
:::
:::

::: {.cell .code id="1xHFskqebwHI"}
``` {.python}
y_testBinarize = label_binarize(y_test, classes=np.unique(y))
y_scoretest = clf.predict_proba(X_test)
```
:::

::: {.cell .code id="BbhzAQ_BbwHI" outputId="5bc6fb94-08d1-4020-eff3-26e795e833e6"}
``` {.python}
# precision recall curve
precision = dict()
recall = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_testBinarize[:, i],
                                                        y_scoretest[:, i])
    plt.plot(recall[i], precision[i], lw=2)
    
plt.xlabel("recall")
plt.ylabel("precision")
#plt.legend(loc="upper left")
plt.title("precision vs. recall curve")
plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/1826b54ce935dbed15f5aa627b9ea4279e96d1b5.png)
:::
:::

::: {.cell .code id="MVkd-HxDbwHI" outputId="49b89f72-7da8-4b03-a650-2f46f6ac5a9d"}
``` {.python}
# roc curve
fpr = dict()
tpr = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_testBinarize[:, i],
                                  (y_scoretest[:, i]))
    plt.plot(fpr[i], tpr[i], lw=2)

plt.xlabel("false positive rate")
plt.ylabel("true positive rate")

plt.title("ROC curve")
plt.show()
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/3abb83cff29d6b08e08d7c2d2f80828f72bbf3ea.png)
:::
:::

::: {.cell .markdown id="ZzpxaLBrbwHJ"}
The model's performance on the test is not that much different from the
training set which shows that the model didn't overfit on the training
set. All the performance metrics are above 97% and the precision vs
recall and ROC curves confirm the performance of the model. The
confusion matrix shows that the model is confusing some Ms for Ws and
vice-versa.
:::

::: {.cell .code id="Nkm7oquVbwHJ" outputId="61710a4d-857a-48bf-d2af-b1cb0d8a2d53"}
``` {.python}
uniqueCharacters = np.unique(y)
uniqueCharacters
```

::: {.output .execute_result execution_count="77"}
    array(['2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E',
           'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T',
           'U', 'V', 'W', 'X', 'Y', 'Z'], dtype='<U1')
:::
:::

::: {.cell .code id="OW16i9FsbwHJ"}
``` {.python}
W = uniqueCharacters[28]
```
:::

::: {.cell .code id="eSpS7ctbbwHK"}
``` {.python}
M = uniqueCharacters[19]
```
:::

::: {.cell .code id="LQnWboXUbwHK" outputId="1ac69f49-7d94-428d-8a0b-a160935d1dea"}
``` {.python}
indexM, = np.where(labels == M)
indexM
```

::: {.output .execute_result execution_count="80"}
    array([   44,    48,    59, ..., 39662, 39670, 39699])
:::
:::

::: {.cell .code id="x5ngAcHobwHK" outputId="8fd45715-8ac9-4347-835f-848d8b1fccd7"}
``` {.python}
indexW, = np.where(labels == W)
indexW
```

::: {.output .execute_result execution_count="81"}
    array([   27,    41,    47, ..., 39718, 39744, 39746])
:::
:::

::: {.cell .code id="j2ohpFCnbwHL" outputId="6a4ade4e-04d3-485a-e534-9d66bcdabf6e"}
``` {.python}
plot_digit(images[indexM[789]])
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/c1143cb92f645fd73c87541ab187441d63a43347.png)
:::
:::

::: {.cell .code id="fT3gDcHdbwHL" outputId="499c910f-54d2-4b84-dd6a-3f83d6a96d72"}
``` {.python}
plot_digit(images[indexW[789]])
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/5935341817aec8ba2aa6411e7cee056d99ea662d.png)
:::
:::

::: {.cell .code id="n0ikJaJdbwHL"}
``` {.python}
cl_a = 'M'
cl_b = 'W'

X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]
X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]

```
:::

::: {.cell .code id="WQRSgc0pbwHL" outputId="7b709b4c-2d60-4bf6-c214-04b0bd7f3604"}
``` {.python}
 plot_digits(X_aa[:24], images_per_row=4)
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/c44d061bc67695fa2de68f01fbfb82dd4c515fb0.png)
:::
:::

::: {.cell .code id="IbYF-C3GbwHL" outputId="73429aa1-1557-404b-9135-8d3247f27b1e"}
``` {.python}
plot_digits(X_ab, images_per_row=7)
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/aa188acd9d1d644310939300bd1cd6a7080800ed.png)
:::
:::

::: {.cell .code id="J6Bnib-4bwHM"}
``` {.python}
X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]
X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]
```
:::

::: {.cell .code id="wjxFe4Y_bwHM" outputId="e3b8d579-8adf-41d0-ea30-2cb06ab68257"}
``` {.python}
plot_digits(X_bb[:25], images_per_row=5)
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/4e9644350b60a39f90935e066d72bb0f377223fc.png)
:::
:::

::: {.cell .code id="80ApnKFrbwHM" outputId="57c092e8-e733-4238-da85-8d9a7db92fa3"}
``` {.python}
plot_digits(X_ba, images_per_row=5)
```

::: {.output .display_data}
![](vertopal_506752d0a93e4361b07a09574f75cb2c/c3e53611170b82e1d438cf9942a35ccae58a116b.png)
:::
:::

::: {.cell .markdown id="Z9UEJMsMbwHM"}
## Images and labels to CSV dataset
:::

::: {.cell .code id="hUmPrx6abwHN" outputId="519f3503-f03f-4187-d80b-fbc183c12c71"}
``` {.python}
X.shape
```

::: {.output .execute_result execution_count="90"}
    (39754, 784)
:::
:::

::: {.cell .code id="tMuP9PYnbwHN" outputId="2198c810-cd90-4547-e38b-a345403e0c55"}
``` {.python}
y[0]
```

::: {.output .execute_result execution_count="97"}
    'G'
:::
:::

::: {.cell .code id="1OQZ3eMWbwHN" outputId="32263788-19b4-4dc9-a3b4-74f2a8c5a3e5"}
``` {.python}
X[0]
```

::: {.output .execute_result execution_count="92"}
    array([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           254, 248, 240, 230, 225, 222, 228, 236, 247, 254, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 254, 237, 216, 189, 176, 171, 186, 206, 236, 252, 254,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 248, 235, 199, 159, 155, 154, 158, 149, 134, 118, 109, 126,
           155, 195, 227, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 254, 252, 235, 203, 138,  72,  85, 106, 140, 137, 114,  67,
            28,  24,  61, 138, 201, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 250, 227, 177,  99, 102, 128, 168, 198, 211, 210,
           201, 172, 136,  74,  83, 163, 214, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 253, 240, 185, 120,  45,  98, 183, 227, 255,
           255, 255, 255, 242, 214, 137, 131, 195, 230, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 249, 222, 115,  66,  73, 148, 235,
           247, 255, 255, 255, 255, 251, 244, 223, 221, 239, 248, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 229, 183,  72,  50, 117,
           188, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 198, 132,  45,
            58, 170, 222, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 174,
            93,  33,  69, 199, 240, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 152,  58,  26,  81, 222, 253, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 140,  40,  32,  94, 226, 254, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 132,  29,  36, 103, 228, 255, 255, 255,
           254, 251, 239, 221, 213, 207, 209, 212, 216, 232, 250, 253, 255,
           255, 255, 255, 255, 255, 255, 255, 130,  27,  36, 103, 228, 255,
           255, 255, 251, 242, 196, 130, 100,  80,  87,  98, 113, 171, 238,
           248, 255, 255, 255, 255, 255, 255, 255, 255, 136,  35,  31,  95,
           225, 254, 255, 255, 252, 244, 207, 153, 101,  56,  38,  56, 109,
           176, 241, 249, 255, 255, 255, 255, 255, 255, 255, 255, 147,  50,
            25,  82, 221, 252, 255, 255, 255, 254, 250, 244, 170,  95,  34,
            57, 166, 219, 253, 254, 255, 255, 255, 255, 255, 255, 255, 255,
           171,  88,  32,  65, 186, 232, 255, 255, 255, 255, 255, 255, 174,
            93,  31,  63, 191, 235, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 198, 131,  46,  50, 144, 205, 253, 254, 255, 255, 255,
           255, 169,  84,  28,  70, 209, 246, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 231, 187,  81,  44,  77, 155, 241, 249, 255,
           255, 255, 255, 161,  72,  26,  74, 218, 251, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 252, 229, 132,  71,  46, 108, 189,
           213, 230, 234, 234, 231, 140,  58,  33,  89, 226, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 254, 245, 208, 151,  74,
            62,  72, 124, 162, 178, 177, 167,  95,  38,  57, 122, 231, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 253, 244,
           208, 145, 112,  89, 116, 136, 145, 147, 144, 116,  98, 130, 177,
           241, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 254, 250, 243, 217, 187, 158, 136, 126, 132, 146, 176, 206,
           230, 246, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 243, 227, 209, 195, 190, 193, 201,
           219, 237, 249, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
           255, 255, 255, 255], dtype=uint8)
:::
:::

::: {.cell .code id="sCVwgGYgbwHN"}
``` {.python}
imagesDF = pd.DataFrame(X)
```
:::

::: {.cell .code id="sP196kUIbwHO" outputId="85f3ac18-6aed-4dfe-c00a-8afa58331ee6"}
``` {.python}
imagesDF.head()
```

::: {.output .execute_result execution_count="103"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>774</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
    </tr>
    <tr>
      <th>1</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
    </tr>
    <tr>
      <th>2</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
    </tr>
    <tr>
      <th>3</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
    </tr>
    <tr>
      <th>4</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 784 columns</p>
</div>
```
:::
:::

::: {.cell .code id="DrzOvXzSbwHO"}
``` {.python}
imagesDF ['labels'] = y
```
:::

::: {.cell .code id="eODplEM1bwHO" outputId="5b3df98e-bba2-4ea8-9fc4-60a91d5cafa3"}
``` {.python}
imagesDF.head()
```

::: {.output .execute_result execution_count="105"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>G</td>
    </tr>
    <tr>
      <th>1</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>U</td>
    </tr>
    <tr>
      <th>2</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>J</td>
    </tr>
    <tr>
      <th>4</th>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>...</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>255</td>
      <td>E</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 785 columns</p>
</div>
```
:::
:::

::: {.cell .code id="rlIPhbjmbwHO"}
``` {.python}
#Saving the clean Dataset
#imagesDF.to_csv('imagesDF.csv', encoding='utf-8')
```
:::
